\chapter{Application} \label{chap:application}

\section{Objectives}

Considering the GLLAMM model developed in previous chapters, its application on a real data set had a four-fold purpose:

\begin{enumerate}
	%
	\item \textbf{Evaluate the performance of the parametrizations.} We assessed if changing the posterior sampling geometry benefited the performance of the MCMC method.
	%
	\item \textbf{Evaluate the retrodictive accuracy.} We wanted to assess how well the model retrodicted the data, and what was the evidence in favor of any of the proposed models.
	%
	\item \textbf{Assess the psychometric properties.} We had a special interest in determine how difficult the items were, and in what part of the abilities measurement range they were located.
	%
	\item \textbf{Test research hypothesis.} We were interested on testing hypothesis about the explanatory power a set of covariates had on the latent dimensions, and what were the implications of these, for the educational authority's policy decision making.
	%
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Instrument}

The evaluation instrument was selected from the Peruvian public teaching career national assessment. The large standardized test took place during $2017$, an it allowed the winner to obtain an appointment, or temporal hiring, into the public teaching career of Peru. 

The instrument was composed of $90$ multiple-choice question, with four alternatives per item. The items were scored on a dichotomous scale, that is, only one of the four alternatives was the correct one. Moreover, the test was organized in three sub-tests designed to evaluate the reading comprehension, mathematical reasoning, and pedagogical knowledge of the applicants. Given the extension and differences between the sub-tests, we decided to focus on the reading comprehension portion, composed of the first $25$ items of the instrument. 

The reading comprehension sub-test was designed to evaluate the teacher's ability to reconstruct the meaning of different types of texts, presented in diverse formats. The sub-test had items designed to measure only one of the three hierarchically nested sub-dimensions of reading comprehension: literal, inferential, and reflective abilities. The literal ability items centered its focus on assessing the teacher's capability to locate explicit information on the texts. The inferential items assessed the teacher's ability to integrate the information in texts, with the goal of inferring its theme, purpose or implicit logic relationships. Lastly, the reflective items evaluated the teacher's abilities to critically reflect about the content and structure of texts.

Finally, besides being nested in dimensions, the items were bundled in groups of five, to a common text or passage, that provided the stimulus over which the individual was assessed, i.e. the items were testlets. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data}

The data set was accessed through the proper legal requirement of open information to the Ministry of Education of Peru (MINEDU). The data was anonymized and transferred through digital mean to the researcher.

Finally, given the large amount of individuals exposed to the aforementioned evaluation (approximately $195,000$), a simple random sample of $2,000$ individuals was taken. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hypothesis} \label{sect:hypothesis}

On it core, statistical models are neat association engines. Through their use, one is able to detect associations between variables, and estimate their effects. However, when a researcher find itself in the position of trying to determine what are the consequences of intervening on a variable, i.e. infer causes, statistical models are never sufficient. Information outside the data, related to the causal hypothesis between the variables, is always required \cite{McElreath_2020}. Nevertheless, since much of time the statistical endeavor has to do with produce understanding that leads to generalization and application; there must be a reasonable way to state our hypothesis, and to think formally about causal inference.

Luckily, as \citet{McElreath_2020}, \citet{Hernan_et_al_2020}, and several other authors indicate, Graphical Causal Models (GCM) can come to the rescue. The simplest and yet powerful GCM is the Directed Acyclic Graph (DAG). A DAG is a heuristic model that contains information that is not purely statistical, but unlike a detailed statistical model, the graph allow us to deduce which variable relationships can provide valid causal inferences. However, abide by the ``no-free lunch" rule, the causal inferences produced under a DAG are only valid if the assumed DAG is correct. On the latter, one would think the aforementioned caveat is an insurmountable critic of the tool, but one would forget that any statistical analysis hinges on assumption, and a DAG is just a tool to make assumptions more transparent. A full description of the GCM's formal theory is beyond the scope of this work. However, if the reader is interested in the topic, he/she can refer to \citet{McElreath_2020} or  \citet{Hernan_et_al_2020} for introductory level descriptions, and beyond, about the use of GCM for the statistical endeavor.

So, for our current research interest, figures \ref{fig:FOLV_app} and \ref{fig:SOLV_app} show the DAGs of the application's first- and second-order latent variable model (FOLV and SOLV, respectively). The figures aim to reflect the hierarchies and hypothesized dimensional structure of the instrument, while also describing the assumed qualitative relationships among the structural covariates. 

A careful inspection of both figures reveal they are similar to the ones implemented in the previous chapter. Therefore, the definitions of the likelihoods, priors, and \textit{hyper-priors} will be similar in nomenclature and even in distributional assumptions, as the simulated counterparts (see appendix \ref{appC3:chapter5}). Nevertheless, the previous cannot be fully extended to the structural covariates, as one can notice the models have a different set of variables than their simulated counterparts. Consequently, in this section we proceed to define them and outline their assumed causal hypothesis.

It is important to emphasize, the hypothesis presented in this section, and the results presented in section \ref{sect:test_hypothesis}, will take a diagnostic perspective, that is, we will emphasize on what factors does the educational authorities will need to give priority, to ensure a fair and equitable development of teachers.
%
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{app_FOLV_dag}
	%
	\caption[Directed Acyclic Graph (DAG). Application's first-order latent variable model (FOLV).]%
	{Directed Acyclig Graph (DAG). Application's first-order latent variable model (FOLV). Circles represent latent variables. Squares represent parameters or parameters for priors. Large squares represent nesting in specific units.}
	\label{fig:FOLV_app}
\end{figure}

First, we assumed the reading abilities and/or sub-dimensions were affected by age ($A$). In this particular case, age was used as a proxy for the applicant's style of teaching. As it is point out by the current National Basic Regular Educational Curriculum of Peru\footnote{National Basic Regular Educational Curriculum (2017). URL: \url{ http://www.minedu.gob.pe/curriculo/pdf/curriculo-nacional-2016-2.pdf} }, approximately forty years ago, an individual was considered literate if he/she had acquired the basic knowledge of reading, writing, and performing mathematics; while having a preliminary exposure to trades's dexterity and abilities. Much of this has been changing throughout the years, and although reading and writing abilities remained important, the criteria to determine if a person is literate, now goes beyond assessing if an individual knows the basic of how to read, write, or apply mathematics. Therefore, it is sensible to assume, age could inform us if a participant has a style of teaching based on the previous requirements of literacy (what we dubbed as the ``old" curricula). The idea is that older applicants, having developed most of their educational careers under the ``old" curricula, now being assessed under the new one, would register lower levels of reading comprehension. However, since the evolution from the ``old" to the new curricula is not a binary scenario, i.e. you either are in one or the other; but rather a continuous evolution, we reflect this idea by allowing age explain the abilities on a continuous manner, more specifically, a linear one. Off course, since age is associated with several other factors, e.g. cognitive development, we understand the previous hypothesis would not be the only one explaining the outcome. However, we believe that by measuring this effect, albeit proxy, educational authorities could be able to target in-training services to individuals with these characteristics.
%
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{app_SOLV_dag}
	%
	\caption[Directed Acyclic Graph (DAG). Application's second-order latent variable model (SOLV).]%
	{Directed Acyclig Graph (DAG). Application's second-order latent variable model (SOLV). Circles represent latent variables. Squares represent parameters or parameters for priors. Large squares represent nesting in specific units.}
	\label{fig:SOLV_app}
\end{figure}

Second, we assumed disability ($D$) causally explained the levels of reading comprehension and/or its sub-dimensions. It is not hard to imagine an scenario where this relationship is true, and a good example of this, is people with vision disabilities. As individuals with vision disabilities are forced to learn completely different set of tools and strategies to be able to read and write, e.g. learn Braille or receive text-to-speech support, the possibility that this additional requirements have hindered the individual's development of the reading comprehension abilities is high. Moreover, the possibility that these effect are reflected on their evaluation outcomes is also highly likely. At this point, it is important to indicate that MINEDU provide the means for people with vision disabilities, to be evaluated under equitable conditions, e.g. using evaluation instruments written on Braille or providing text-to-speech support. However, we believe the effects of the disability could offset the effects of the conditions set in place to ensure an equitable evaluation, but we do not know by how much. Furthermore, although the justification to include this variable in the model have been guided by the previous example, one cannot discard that other types of disabilities could also had an impact on the development of the reading comprehension abilities. In this sense, we believe the educational authority only benefits from obtaining this type of evidence, to either target in-training services for these individuals, or to improve further their evaluations conditions, to ensure their participation on the educational workforce. 

Third, we assumed the type of education an applicant received ($E$) affected the reading comprehension abilities, where type of education was a categorical variable that denoted if an individual received his/her pedagogical training from an institute, university, or both. The assumptions behind the inclusion of the variable derives from incidental evidence about the quality of training on pedagogical institutes. In Peru, there is silent, but widely accepted notion that most pedagogical institutes produce teacher with low levels of pedagogical abilities. Multiple reason have been considered, that is, a poorly devised or outdated curricula of teaching; their lack of funding; the fact that mostly institutes are specialized on training people with disabilities; or the fact that because institutes are easier to enter and less expensive than private universities, people coming the lower tiers of income (another proxy of educational opportunities and development), select institutes to begin their education. Considering the previous, it could be of interest for the educational authority to set this incidental evidence on a more quantitative ground, again with the purpose of providing remedial measures. 

At this point, it is important to indicate that from the previous paragraph statements, one can easily assume a backdoor path exists between the education and disability variables (see the path $D \rightarrow E$ on the model's figures). A backdoor path is just a DAG's path that outlines a relationship between two variables, that if remains uncontrolled for, can confound the variables' effects on an outcome. In our specific case, this just means that, in order to obtain unbiased estimates for the effects of education and/or disability, both variables need to considered in the model, to close any of the backdoor paths.

Fourth, we assumed the educational career experience, measured in years of public or private teaching ($X_{pu}$ and $X_{pr}$, respectively), affected the reading comprehension abilities. In this case, the causal assumption goes in line with the idea that with more teaching experience, the teacher had more opportunities to develop key aspects of the reading comprehension abilities. However, as it happened previously, we believe a backdoor path exist between these covariates and the educational variable. The intuition of these relationships derives from the same incidental evidence relating the type and quality of the pedagogical training. One can imagine, if the pedagogical training in institutes is perceived as of lesser quality, the opportunities a teacher can access, if any, will also be of less quality. We believe the latter is especially true in the private sector, where this perception can also be translated in less years of experience. However, since we are interest in estimating the unbiased and independent effects of education and experience, both set of variables need to be included in the model, as they already are.

Fifth, we assumed the broad definition of teaching specialty ($S$) also explained causally the reading comprehension abilities and/or its sub-dimensions. Specialty was a categorical variable denoting if a teacher had the credentials to instruct at the early childhood, primary, or secondary educational levels. The intuition behind its inclusion, lies in the idea that instructing students that require more complex levels of written and oral communication, could end up benefiting the reading comprehension abilities of teachers. 

Notice from the model's figures, that we additionally assumed, there is a backdoor path between the specialty and gender variables. In this case, this relationship only tries to reflect that it is more likely that women choose to specialize on the early childhood and primary educational levels. However, it is important to indicate that gender only enters the model as a control variable, following a common practice in the literature.

Therefore, after declaring our hypothesis, we proceed to declare the prior model assumptions related to the aforementioned covariates. We state our current lack of knowledge about the effects of the variables, by setting weakly informative priors, in the following way: 
%
\begin{align}
	\Gamma_{1} &\sim \text{Normal}( 0, 0.5 ) \\
	\Gamma_{2c} &\sim \text{Normal}( 0, 0.5 ) \\
	\Gamma_{3c} &\sim \text{Normal}( 0, 0.5 ) \\
	\Gamma_{4c} &\sim \text{Normal}( 0, 1 ) \\
	\Gamma_{5c} &\sim \text{Normal}( 0, 0.5 ) \\
	\Gamma_{6c} &\sim \text{Normal}( 0, 0.5 )  \\
	\Gamma_{7c} &\sim \text{Normal}( 0, 0.5 ) 
	%
\end{align} 

\noindent where as in the previous chapter, $c$ denotes the categories inside the covariate, different for each variable. 

Finally, by no means we state the proposed model is the ``correct" model to analyze the data. This is particularly true, if we consider that several unobserved important variables are absent from the analysis, e.g. if the individual followed training courses beyond their initial education, or something as simple as, the amount of hours the individual spends on reading. Moreover, this is aggravated by the fact that a good portion of the variables used in the analysis come from self-reported measures, like experience and education. However, we believe that giving the constraints of the data, the model provides a sensible depiction of the causal hypothesis among the available variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Parametrization performance} \label{sub_sect:ergodicity_application}

Figures \ref{fig:FOLV_CE_chains1} and \ref{fig:FOLV_NC_chains1} show the item parameters' trace, trank, and ACF plots for the centered and non-centered parametrization of the FOLV model. From a quick inspection of the figures, we realize the items' parameter did not achieved ergodicity under the CP, in contrast to the NCP. This result if further confirmed by figure \ref{fig:FOLV_stat1}, where we observe the items parameters had effective sample sizes below $100$ under the CP, while above $1,500$ under the NCP. Moreover, the parameters' chains registered \texttt{Rhat} values well above the recommended threshold under CP, but maintained values close to one under the NCP.
%
\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{FOLV_CE_bk_1_5}
	%
	\caption[Application's first-order latent variable model (FOLV). Centered parametrization. Items difficulty. Trace, trank and auto-correlation plots.]%
	{Application's first-order latent variable model (FOLV). Centered parametrization. Items difficulty: (Left) trace plot, (Middle) trank plot, (Right) auto-correlation plot.}
	\label{fig:FOLV_CE_chains1}
\end{figure}

A similar behavior was registered for the texts difficulties and deviations, and individuals abilities. From the inspection of the appropriate figures\footnote{To inspect the figures refer to the github accompanying page detailed in Appendix \ref{app:chapter5}.}, we notice the CP did not achieved stationarity, convergence, nor good mixing, albeit the patterns were less extreme than in the previous scenario. In contrast, the NCP achieved ergodicity without any issues. Again, this was further confirmed by the comparison of the \texttt{n\_eff} and \texttt{Rhat} values between the two parametrizations.
%
\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{FOLV_NC_bk_1_5}
	%
	\caption[Application's first-order latent variable model (FOLV). Non-centered parametrization. Items difficulty. Trace, trank and auto-correlation plots.]%
	{Application's first-Order latent variable model (FOLV). Non-centered parametrization. Items difficulty: (Left) trace plot, (Middle) trank plot, (Right) auto-correlation plot.}
	\label{fig:FOLV_NC_chains1}
\end{figure}

Moreover, both parametrizations had a hard time exploring the posterior distribution of the sub-dimensions correlations. Inspecting the appropriate figures (not shown), we noticed the CP registered effective sample sizes below $100$, while the NCP barely reached sample sizes above $300$. This, in conjunction with the trace, trank, and ACF plots further confirmed the chains did not achieved ergodicity.
%
\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{FOLV_stat_bk}
	%
	\caption[Application's first-order latent variable model (FOLV). CP and NCP comparison plot.]%
	{Application's first-order latent variable model (FOLV). CP and NCP comparison plot. (A) \texttt{n\_eff} for items' difficulties. (B) \texttt{Rhat} for items' difficulties. Diagonal discontinuous line describes equality between CP and NCP. Vertical and horizontal discontinuous lines set in A corresponds to \texttt{n\_eff}$=100$. Vertical and horizontal discontinuous lines set in B corresponds to \texttt{Rhat}$=1.05$.}
	\label{fig:FOLV_stat1}
\end{figure}

Strikingly similar patterns were observed in the same set of parameters the SOLV shares with the FOLV model. Part of the evidence of such statement can be found in figures \ref{fig:SOLV_CE_appchains1} and \ref{fig:SOLV_NC_appchains1} (appendix). Moreover, the performance patterns of the loadings were similar to the ones observed for the correlations.

All of the preceding results resonate with the performance patterns obtained in the previous chapter. However, in contrast with the results of the simulation study, some of the structural regression parameters (not shown) registered better performance under the CP, rather than the NCP. Among these set of covariates were age, disability, and both of the experience variables. Nevertheless, the performance improvement was not unanimous, as the parameters showed ``healthier" chains with larger effective sample sizes, but \texttt{Rhat} values above the recommended threshold, indicating a lack of convergence. This result resonates with \citet{Papaspiliopoulos_et_al_2007}, who stated that the success of the NCP strategy was largely dependent on the specifics of the models and data. Moreover, it is surprising this pattern of behavior was not replicated on the SOLV model, where all of the structural parameters seemed to be more ergodic under the NCP. The latter lead us to think, the previous patterns resulted from the use of a possibly miss-specified model.

In conclusion, given the results obtained in this section, and section  \ref{sub_sect:ergodicity_simulation} of the previous chapter, conditional on the selected number of iterations, the non-centered parametrization was the only one to ensure the GLLAMM parameters attained ergodicity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Retrodictive accuracy} \label{sub_sect:retrodictive_application}

Much similar to what was the observed in section \ref{sub_sect:retrodictive_simulation} of the simulation study, we notice the model manages to capture the traits of the data, while avoiding its exact replication. Figure \ref{fig:FOLV_pred_app} show the individuals' true proportion of endorsed items, which are within the compatibility intervals of the average and marginal predictions, that is, the observed values for the proportions were close to the predicted mean and outcomes produced by the model. Similarly, we also observe the average and marginal predictions show some level of shrinkage, i.e. the estimates were ``pulled" towards the average true proportion across individuals, in greater or less quantity. As explained in the simulation chapter, this results from the complex pooling of information across individuals, items, texts and dimensions; that allows the model to negotiate between predicting all individuals with the average proportion (under-fitting) or predicting each individual by its own proportion (over-fitting). 
%
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{FOLV_NC_HitRate_ind}
	%
	\caption[First-order latent variable model (FOLV). Non-centered parametrization. Individual predictive plot.]%
	{First-order latent variable model (FOLV). Non-centered parametrization. Individual predictive plot.}
	\label{fig:FOLV_pred_app}
\end{figure}

In a similar fashion, inspecting the appropriate figures (not shown), we can say the retrodictive accuracy of the FOLV model extends to the predicted proportions per individual and covariate combination, as well as proportions aggregated by dimensions, items, and texts\footnote{To inspect the figures refer to the github accompanying page detailed in Appendix \ref{app:chapter5}.}. Furthermore, the SOLV model managed to do a similar job in all of the previous descriptions, as it is evident from figure \ref{fig:FOLV_pred_app}.

Finally, regarding the retrodictive accuracy and model fit, what remains to determine is if the statistical evidence favors one model over the other. This is particularly important, if we notice the SOLV model have approximately $2,000$ more parameters than the FOLV, that is, the reading comprehension abilities of the individuals (not the sub-dimensions). In that sense, a lurking fear of over-fitting the data with the SOLV model is present. Additionally, since we implemented our model in a Bayesian framework, the uncertainty of the model fit now has two faces, one resulting from the uncertainty of the parameters, and one coming from to uncertainty of the outcome's likelihood. Therefore, a proper assessment of a model fit would needs to consider these two facets. Lastly, since comparing models based on their training data fit could lead us to wrongful conclusions, we need a method that evaluates our models out-of-sample or through cross-validation, i.e. on data not used to train the model's parameters.

Luckily, as \citet{McElreath_2020} indicate, is a benign fact of the statistical theory, that we do not need to re-fit the model multiple times to obtain approximately valid cross-validation measures. Moreover, he continues, is also useful that there are statistics that uses all the information of a ``model's distribution" to asses its fit, and this measures are called information criteria.

It is safe to say that the theory behind the Kullback-Leibler divergence (KL divergence) \cite{Kullback_et_al_1951}, and the field of of information theory in general, from which the information criteria derives, is way out of the scope of the current research. However, the reader can rest assure the information criteria fulfills all the previous requirements to make the proper comparison of our models, more specifically the Pareto-smoothed importance sampling cross-validation (PSIS) \cite{vehtari_et_al_2021} and the Widely Applicable Information Criterion (WAIC) \cite{Watanabe_2013}. Additionally, the reader should keep in mind that as these measures try to approximate the out-of-sample KL divergence, is not the absolute magnitude that matters, but the difference among them. 
%
\begin{table}[H]
	\centering
	\begin{tabular}{rlrrrr}
		\hline
		& Model & Parametrization & WAIC & lppd & penalty \\  
		\hline\hline
		1 & FOLV & CP &  54,632.4 & -25,429.7 & 1,886.5 \\ 
		2 & FOLV & NCP & 54,631.7 & -25,427.6 & 1,888.3 \\
		%
		\hline
		%
		3 & SOLV & CP &  54,610.3 & -25,348.4 & 1,956.7 \\  
		4 & SOLV & NCP & 54,614.7 & -25,337.9 & 1,969.4 \\ 
		\hline
	\end{tabular}
	\caption[Model fit. Widely Applicable Information Criterion (WAIC).]%
	{Model fit. Widely Applicable Information Criterion (WAIC).}
	\label{tab:model_fit1}
\end{table}
%
\begin{table}[H]
	\centering
	\begin{tabular}{rlrrrr}
		\hline
		& Model & Parametrization & PSIS & lppd & penalty \\  
		\hline\hline
		1 & FOLV & CP &  54,657.4 & -27,328.7 & 1,904.4 \\ 
		2 & FOLV & NCP & 54,656.9 & -27,328.5 & 1,898.1  \\
		%
		\hline
		%
		3 & SOLV & CP &  54,627.3 & -27,313.7 & 1,940.1 \\  
		4 & SOLV & NCP & 54,642.5 & -27,321.2 & 1,990.2 \\ 
		\hline
	\end{tabular}
	\caption[Model fit. Pareto-smoothed importance sampling cross-validation (PSIS).]%
	{Model fit. Pareto-smoothed importance sampling cross-validation (PSIS).}
	\label{tab:model_fit2}
\end{table} 

Tables \ref{tab:model_fit1} and \ref{tab:model_fit2} show the WAIC and PSIS statistics, respectively. From the tables we notice all models and parametrizations seem to achieve similar levels of out-of-sample fit. Moreover, the penalties of the statistics indicate all the models have similar levels of over-fitting risk, i.e. all models have a similar risk of encoding too much of the data. As previously mentioned, this is important, because the difference between one model and the other is about $2,000$ parameters.

Consequently, from the results, wa can safely say that both models produced similar encodings of the data. Therefore, the decision of choosing one over another rest now on a more theoretical ground.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{figure}[H]
	\centering
	\begin{subfigure}
		\includegraphics[width=0.9\linewidth]{FOLV_recovery_items}
	\end{subfigure}
	%
	\begin{subfigure}
		\includegraphics[width=0.87\linewidth]{FOLV_recovery_texts}
	\end{subfigure}
	%
	\caption[Application's first-order latent variable model (FOLV). Centered and non-centered parametrization. Items, and texts difficulties, and texts deviations.]%
	{Application's first-order latent variable model (FOLV). Centered and non-centered parametrization. Items, texts difficulties, and texts deviations.}
	\label{fig:FOLV_CE.NC_recovery}
\end{figure}

\subsection{Psychometric properties}

As in any standardized evaluation, instrument developers have a special interest in determine how difficult the items were, and in what part of the abilities measurement range they were located. From the top panel of figure \ref{fig:FOLV_CE.NC_recovery}, we notice that the items were scattered throughout a significant portion of the abilities range. However, we also notice the items were more prevalent on the lower tiers, with items located as low as $2.0$ logits.

Furthermore, an specific benefit of the current implementation also allowed us to assess how difficult the texts were. The bottom left panel of figure \ref{fig:FOLV_CE.NC_recovery} show the texts difficulties we located around $-0.5$ logits, with large confidence intervals, indicating the items within the texts were scatter in a wide range around the text mean.

It is important to mention, that in order to provide a sound analysis of the psychometric characteristics of the items, and what they imply for the instruments developers, one would need to have a access to the items and texts description. However, in this case, it was not possible due to the legal restrictions surrounding this information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Test hypothesis} \label{sect:test_hypothesis}

Finally, we proceed to assess the statistical evidence behind the hypothesis stated in section \ref{sect:hypothesis}. It is important indicate that, more than having an interest in the actual parameter levels, e.g. the reading comprehension estimate for males and females, we had an interest in the contrast between the parameters, that is, the difference between females and males. The reason for the latter rest on the fact that, testing the hypothesis for the levels would correspond to test how far those categories are from zero, much like in the usual frequentist null hypothesis testing ($H_{0}: \beta=0$). However, in our case, zero does not carry any critical significance for the levels, because we have arbitrarily assumed the individual's latent variables are normally distributed with mean zero (the intercept) and variance one. On the contrary, for our purposes., testing if the difference between two levels is zero ($H_{0}: \beta_{2} - \beta_{1}=0$) does carry inference value.
%
\begin{figure}[H]
	\centering
	\begin{subfigure}
		\includegraphics[width=0.85\linewidth]{FOLV_recovery_contrast}
	\end{subfigure}
	%
	\begin{subfigure}
		\includegraphics[width=0.85\linewidth]{SOLV_recovery_contrast}
	\end{subfigure}
	%
	\caption[Application's first- and second-order latent variable model. CP and NCP comparison plot.]%
	{Application's first- and second-order latent variable model. CP and NCP comparison plot. (Top panel) Contrasts in the FOLV model. (Bottom panel) Contrasts in the SOLV model. }
	\label{fig:contrast_both}
\end{figure}

Figure \ref{fig:contrast_both} show the full set of contrast resulting from the implemented model. The first thing to notice, between the FOLV and SOLV models, is the difference in scale on which the effects are measured. We notice the evidence in favor or against a hypothesis is larger under the SOLV model. We argue this is due to the issues related with the estimation of the loading and correlations, which in this case, followed the results obtained in the previous chapter. As the reader can recall, in the previous chapter we observed that for larger sample sizes, the SOLV model estimated low loadings but kept higher levels of correlations; and the same pattern have been replicated in our application, i.e. the estimated loadings are around $0.3$, with correlations around $0.9$. The reasons for these results have escaped explanation, and more careful analysis is required. However, is not hard to realize in this context that, if the effects of the covariates ultimately need to ``explain" the responses, and these effects need to pass through the latent variable structure, given the low estimates of the loadings, larger effects on the second-order latent variable would be required. Given the lack of confidence the previous results generates on the magnitude of our covariate estimates, from this point onward, we will interpret our statistical evidence based on the FOLV model.

First, there is sufficient statistical evidence that age explains negatively the levels of reading comprehension; more specifically, the sub-dimensions of reading comprehension (not shown in the plot). The estimate for the age parameter was $-0.036$ with a $95\%$ compatibility interval of $[-0.043, -0.031]$. This mean that, for each additional unit of age, the applicant departed from the minimum registered in the data ($20$ years), his/her abilities to find literal information, infer and reflect over a text, get diminished. Although the effect does not seem large on a unit scale, notice that a difference of $10$ years among participants would represent nearly $0.7$ logits of difference in abilities. As previously stated, this not conclusive evidence that the style of teaching affects the reading comprehension abilities. However, it does provide evidence that older individual inside the teaching career, could benefit from in-training services to improve their reading comprehension abilities.

Second, there is also evidence that disability explained the variability in the reading comprehension sub-dimensions, although the results were mildly unexpected. From the figure we noticed the contrast between having a low vision and no disability was positive (\texttt{b\_Di[2] - b\_Di[1]} $=0.38 \; [0.051, 0.724]$). This is surprising as one would expect that people with low vision would be in disadvantage. However, a different story could be told. What could had happen is that because people had low vision, they read the exam more carefully, and therefore, were able to obtain better results. The current evidence seem to support the latter. Another unexpected result was that, individuals with low motor skills performed largely worse than individuals with no disability or low vision, as one can observe from the second and third labels of the plot. While the reason for this result is not puzzling, since individual have a disability after all, the actual mechanisms behind it are not entirely clear. Therefore, the educational authority could make a proper investigation of this evidence. Finally, the statistical evidence did not reject the notion that having an auditory disability, does not affect the reading comprehension abilities. Notice the large confidence intervals in the next three plot labels. However, we argue that the impossibility of getting a proper hypothesis test for these contrast, had more to do with the sample design, rather than the true effect being null. As one can intuit, a simple random sample would likely reflect most of the traits of a full data set, however, it is also clear that this design does not necessarily favors the hypothesis testing of contrast, especially, in cases where the groups of interest have low representativeness.

Third, the statistical evidence seem to support the incidental observation, about the quality of training on pedagogical institutes. From the figure we notice that, individual with university degrees had higher levels of reading comprehension abilities, versus individuals with institute degrees (\texttt{b\_E[2] - b\_E[1]} $=0.260 \; [0.180, 0.340]$). Moreover, we observed that individuals with both degrees were not that different from individuals with institute degrees (\texttt{b\_E[3] - b\_E[1]} $=0.099 \; [-0.151, 0.355]$). The latter makes sense, if we consider that it is more likely that individuals graduated from institutes, further develop their education on universities, and not the other way around. However, the implications of this reverberate on two additional hypothesis, the educational authority might be interested on investigate: (i) either the negative offset of starting your pedagogical training on an institute is never recovered, even with the assumed ``better" education from universities, or (ii) people from institutes further develop their education on ``bad" quality universities. Nevertheless, considering all the previous, we must not forget the evidence presented here only speaks about the reading comprehension abilities, that is, evidence on other abilities need to be assessed. Moreover, we also need to consider the effects might be less or more impressive, once we consider the variability present within institutes.

Fourth, from the last $12$ contrast on the top panel of figure \ref{fig:contrast_both}, we see there is statistical evidence that experience improved the reading comprehension abilities. Additionally, we can notice the effects of private experience (\texttt{b\_Xpr}) were larger than their public counterpart (\texttt{b\_Xpu}). Finally, on both types of experiences, we can observe a pattern of diminishing returns on abilities, as the years of experience increases.

Fifth and final, the statistical evidence seem to favor the idea that instructing students, which require more complex levels of written and oral communication, might be benefiting the reading comprehension abilities of teachers. However, the hypothesis is favored only for teacher involved in the secondary educational level (notice the contrast \texttt{b\_S[3] - b\_S[1]} and \texttt{b\_S[3] - b\_S[2]}).
