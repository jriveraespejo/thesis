\chapter{Code} \label{appC:additional}

This section provides the most important set of codes used throughout the current research. However, in case the reader wants to replicate the full simulation analysis, we provide the full set of codes in the ``simulation" section of the accompanying github page:

\noindent \url{https://github.com/jriveraespejo/thesis/tree/master/simulation} \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chapter 3: Bayesian estimation} \label{appC1:chapter3}

\subsection{To center or not to center} \label{appC1_1:noncenter}

\subsubsection{The devil's funnel, centered parametrization.} 

\noindent \textbf{Stan}
%
\begin{lstlisting}
transformed data {
	int<lower=0> J;
	J = 1;
}
parameters {
	real theta[J];
	real v;
}
model {
	v ~ normal(0, 3);
	theta ~ normal(0, exp(v));
}
\end{lstlisting}


\noindent \textbf{JAGS}
%
\begin{lstlisting}
model{
	v ~ dnorm(0,3)
	theta ~ dnorm(0, exp(v))
}
\end{lstlisting}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The devil's funnel, centered parametrization with priors.} 

\noindent \textbf{Stan}
%
\begin{lstlisting}
transformed data {
	int<lower=0> J;
	J = 1;
}
parameters {
	real theta[J];
	real v;
}
model {
	v ~ normal(0, 1);
	theta ~ normal(0, exp(v));
}
\end{lstlisting}


\noindent \textbf{JAGS}
%
\begin{lstlisting}
model{
	v ~ dnorm(0,1)
	theta ~ dnorm(0, exp(v))
}
\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The devil's funnel, non-centered parametrization.}

\noindent \textbf{Stan}
%
\begin{lstlisting}
transformed data {
	int<lower=0> J;
	J = 1;
}
parameters {
	real ztheta[J];
	real v;
}
transformed parameters{
	vector[J] theta;
	theta = exp(v) * to_vector( ztheta );
}
model {
	v ~ normal(0, 3);
	ztheta ~ normal(0, 1);
}
\end{lstlisting}


\noindent \textbf{JAGS}
%
\begin{lstlisting}
model{
	v ~ dnorm(0,1)
	ztheta ~ dnorm(0,1)
	theta = v * ztheta
}
\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Chapter 4: Simulation study} \label{appC2:chapter4}

\subsection{Algorithm} \label{appC2_1:sim}

\noindent \textbf{Data generation}
%
\begin{lstlisting}
S = 10 # ten data sets
condition = expand_grid( J = c(100, 250, 500), load=0.95)
for(i in 1:nrow(condition)){
	for(s in 1:S){
		with(condition[i,],
			data_generation( J=J, loads=rep(load, 3), 
				Ndata=s, seed=4587+s+i, # different seeds
				file_dir=file.path(getwd(), 'data') ) )
	}
}
\end{lstlisting}

\noindent \textbf{Data generation function}
%
\begin{lstlisting}
# function:
#     data_generation
# description:  
#     To generate data based on different parameter settings.
#     Only two parameters are effectively controlled in the experimentation: 
#     sample size (J), and the loading from the SOLV to the FOLV (loads).
# characteristics of the sample design:
#   - one instrument
#   - hierarchical measurement scales with FOLV and SOLV
#   - one evaluation time
#   - one sample of individuals
#   - testlet items, multiple items come from one text
#   - with covariates
#   - NO missingness
# arguments:
#     J = individual sample sizes
#     loads = loadings from SOLV to FOLV (it control correlation)
#     Ndata = defines the number of data generated
#     file_dir = path to save generated data
#     s_theta = sd to simulate SOLV and FOLV
#     s_text = sd in generating items from a specific text
#     D = number of dimensions (default 3)
#     K = number of items (default 25)
#     L = number of texts (default 5), it has to be a multiple of K
#     seed = seed used to generate the simulation (default 1)
#     prec = rounding in abilities and item parameters (default 3)
	
data_generation =function( J=100, loads=rep(0.95, 3), Ndata=1, file_dir,
	s_theta=0.5, s_text=0.5, D=3, K=25, L=5, seed=1, prec=3){
		
#_______________
# 1. generation 
#_______________
set.seed(seed)
		
	
## 1.1. regression parameters
mom = expand_grid(a=loads[-3], b=loads[-1])
mom = mom[-3,]
		
betas = list( gender = c(0, 0.5),
	age = -0.02,
	edu = c(-0.5, 0.5, 0),
	exp = c(-0.5, 0, 0.35, 0.5),
	loads = loads, # loadings
	exp_corr = with(mom, a*b) ) # expected correlation
		
		
## 1.2 covariates
abilities = data.frame( IDind=1:J,
	gender = sample(c(1,2), size=J, replace=T),
	age = sample(30:65, size=J, replace=T),
	edu = sample(c(1,2,3), size=J, replace=T),
	exp = sample(c(1,2,3,4), size=J, replace=T),
	theta=rep(NA,J), theta1=rep(NA,J), 
	theta2=rep(NA,J), theta3=rep(NA,J) )
	
# variable indices
SOLV_loc = which(names(abilities)=='theta')
FOLV_loc = which(names(abilities)=='theta1'):
	which(names(abilities)==paste0('theta', D))
		
		
## 1.3. abilities
# SOLV
m_theta = rep(NA, J)
for(j in 1:J){
	ageC = with(abilities, age[j] - min(age) + 1 )
	
	m_theta[j] = with(abilities,
		betas$gender[ gender[j] ] + 
		betas$age * ageC +
		betas$edu[ edu[j] ] + 
		betas$exp[ exp[j] ] )  
}
abilities[, SOLV_loc] = round( rnorm(J, m_theta, s_theta), prec)
		
# FOLV
# independent after considering SOLV
s_mult = diag( rep(s_theta, D) ) 
m_mult = data.frame( theta1=rep(NA, J), 
	theta2=rep(NA, J), 
	theta3=rep(NA, J) )
for(j in 1:J){
	m_mult[j,] = betas$loads * abilities$theta[j]
	abilities[j, FOLV_loc] = round( 
		mvrnorm(n=1, mu=unlist(m_mult[j,]), Sigma=s_mult), prec)
}
		

## 1.4 texts
texts = data.frame(IDtext=1:L, m_b=rep(NA, L), s_b=rep(NA, L))
texts$m_b = seq(-1.5, 1.5, length.out=L)
texts$s_b = rep(s_text, L)
		
		
## 1.5 items
items = data.frame(IDitem=1:K, IDtext=rep(NA,K), 
	IDdim=rep(NA,K), b=rep(NA, K))
kl = K/L # items per text
for(k in 1:nrow(texts)){
	items$b[(1:kl) + (k-1)*kl] = with(texts, 
		round( rnorm(kl, m_b[k], s_b[k]), prec ) )
	items$IDtext[(1:kl) + (k-1)*kl] = k
}
		
		
## 1.6 dimensions
items$IDdim = sample(1:3, K, replace=T)
		
#_______________
# 2. storage
#_______________
		
## 2.1 parameters
data_true = list(
	
	seed=seed,
	
	# indices
	J = J,
	D = D, 
	K = K, 
	L = L,
		
	# abilities
	betas = betas,
	abilities = abilities,
		
	# texts and items
	texts = texts,
	items = items)
	
	file_name = paste0('Parameters_J',J,'_l',loads[1],
		'_Ndata',Ndata,'.RData')
	save(data_true, file=file.path(file_dir, file_name) )
		
		
		
## 2.2 long format
data_eval = data.frame(
		
	# individual data
	IDind = rep(abilities$IDind, K),
	gender = rep(abilities$gender, K),
	age = rep(abilities$age, K),
	edu = rep(abilities$edu, K),
	exp = rep(abilities$exp, K),
	theta = rep(abilities$theta, K),
	theta1 = rep(abilities$theta1, K),
	theta2 = rep(abilities$theta2, K),
	theta3 = rep(abilities$theta3, K),
		
	# items
	IDitem = rep(items$IDitem, each=J),
	IDtext = rep(items$IDtext, each=J),
	IDdim = rep(items$IDdim, each=J),
	b = rep(items$b, each=J) )
		
# appropriate ability
data_eval$theta_jkld = NA
for(i in 1:nrow(data_eval) ){
	data_eval$theta_jkld[i] = data_eval[i, FOLV_loc[ data_eval$IDdim[i] ] ]
}
		
# linear predictor, probability, and outcome
data_eval$v_jkld = with(data_eval, theta_jkld - b)
data_eval$p_jkld = inv_logit( data_eval$v )
data_eval$y_jkld = rbinom( nrow(data_eval), 1, prob=data_eval$p)
		
file_name = paste0('LongFormat_J',J,'_l',loads[1],
	'_Ndata',Ndata,'.RData')
save(data_eval, file=file.path(file_dir, file_name) )
		
	
## 2.3 estimation data
data_post = list(
		
	# evaluation data
	N = nrow(data_eval),
	J = J,
	K = K,
	L = L,
	D = D,
	IDj = data_eval$IDind,
	IDk = data_eval$IDitem,
	IDl = data_eval$IDtext,
	IDd = data_eval$IDdim,
	GE = data_eval$gender,
	AG = data_eval$age,
	ED = data_eval$edu,
	XP = data_eval$exp,
	y = data_eval$y_jkld,
		
	# individual data
	IDind = abilities$IDind,
	G = abilities$gender,
	A = abilities$age,
	E = abilities$edu,
	X = abilities$exp,
		
	# items
	IDitem = items$IDitem,
	IDtext = items$IDtext,
	IDdim = items$IDdim )
		
file_name = paste0('ListFormat_J',J,'_l',loads[1],
	'_Ndata',Ndata,'.RData')
save(data_post, file=file.path(file_dir, file_name) )
		
}
\end{lstlisting}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results} \label{appC2_2:results}

\subsubsection{Models}

it has the stan model used to analize the data \\

\noindent \textbf{FOLV CP}
%
\begin{lstlisting}
data{
	// evaluation
	int N;
	int J;
	int K;
	int L;
	int D;
	int IDj[N];
	int IDk[N];
	int IDl[N];
	int IDd[N];
	int GE[N];
	real AG[N];
	int ED[N];
	int XP[N];
	int y[N];
	
	// individuals
	int IDind[J];
	int G[J];
	real A[J];
	int E[J];
	int X[J];
	
	// items
	int IDitem[K];
	int IDtext[K];
	int IDdim[K];
}
parameters{
	// items
	real m_b[L];
	real<lower=0> s_b[L];
	real b_k[K];
	
	// betas
	real a;
	real b_G[2];
	real b_A;
	real b_E[3];
	real b_X[4];
	
	// abilities
	// sub-dimensions: lit, inf, ref
	corr_matrix[D] Rho_theta_sub; 
	vector[D] theta_sub[J]; 
}
model{
	// declare
	vector[D] m_mult[J];
	real v;
	real p;
	
	// items
	m_b ~ normal(0, 1);
	s_b ~ exponential(2);
	for(k in 1:K){            // priors
		b_k[k] ~ normal( m_b[ IDtext[k] ], s_b[ IDtext[k] ]); 
	}
	
	// abilities
	a ~ normal(0, 0.5);
	b_G ~ normal(0, 0.5);
	b_A ~ normal(0, 0.5);
	b_E ~ normal(0, 1);
	b_X ~ normal(0, 0.5);
	Rho_theta_sub ~ lkj_corr(2);
	for(j in 1:J){
		m_mult[j,] = rep_vector( a + b_G[ G[j] ] +
			b_A * ( A[j] - min(A) ) +
			b_E[ E[j] ] + 
			b_X[ X[j] ], D);
		// using the same predictor for the three latents
	}
	theta_sub ~ multi_normal( m_mult, Rho_theta_sub );
	
	// model
	for( i in 1:N ) {
		v = theta_sub[ IDj[i], IDd[i] ] - b_k[ IDk[i] ];
		p = inv_logit(v);
		y[i] ~ bernoulli(p);
	}
}
//# generated quantities{
//#     vector[N] log_lik;
//#     real v;
//#     real p;
//#     
//#     // likelihood
//#     for( i in 1:N ) {
//#       v = theta_sub[ IDj[i], IDd[i] ] - b_k[ IDk[i] ];
//#       p = inv_logit(v);
//#       log_lik[i] = bernoulli_lpmf( y[i] | p);
//#     }
//# }
\end{lstlisting} 


\noindent \textbf{FOLV NCP}
%
\begin{lstlisting}
data{
	// evaluation
	int N;
	int J;
	int K;
	int L;
	int D;
	int IDj[N];
	int IDk[N];
	int IDl[N];
	int IDd[N];
	int GE[N];
	real AG[N];
	int ED[N];
	int XP[N];
	int y[N];
	
	// individuals
	int IDind[J];
	int G[J];
	real A[J];
	int E[J];
	int X[J];
	
	// items
	int IDitem[K];
	int IDtext[K];
	int IDdim[K];
}
parameters{
	// items
	real m_b[L];
	real<lower=0> s_b[L];
	real zb_k[K];
	
	// betas
	real a;
	real b_G[2];
	real b_A;
	real b_E[3];
	real b_X[4];
	
	// abilities
	// sub-dimensions: lit, inf, ref
	cholesky_factor_corr[D] L_Rho_theta_sub; 
	matrix[D, J] ztheta_sub; 
}
transformed parameters{
	real b_k[K];
	matrix[D, D] Rho_theta_sub;
	matrix[J, D] m_mult;
	matrix[J, D] theta_sub;
	
	// items
	for(k in 1:K){
		b_k[k] = m_b[ IDtext[k] ] + s_b[ IDtext[k] ] * zb_k[k]; 
	}
	
	// abilities
	Rho_theta_sub = multiply_lower_tri_self_transpose(L_Rho_theta_sub);
	for(j in 1:J){
		m_mult[j,] = rep_row_vector( a + b_G[ G[j] ] + 
			b_A * (A[j] - min(A) ) +
			b_E[ E[j] ] + 
			b_X[ X[j] ], D);
	}
	theta_sub = (L_Rho_theta_sub * ztheta_sub)';
	theta_sub = theta_sub + m_mult;
}
model{
	// declare
	real v;
	real p;
	
	// items
	m_b ~ normal(0, 1);
	s_b ~ exponential(2);
	zb_k ~ normal(0, 1);
	
	// abilities
	a ~ normal(0, 0.5);
	b_G ~ normal(0, 0.5);
	b_A ~ normal(0, 0.5);
	b_E ~ normal(0, 1);
	b_X ~ normal(0, 0.5);
	L_Rho_theta_sub ~ lkj_corr_cholesky(2);
	to_vector(ztheta_sub) ~ normal(0,1);
	
	// model
	for( i in 1:N ) {
		v = theta_sub[ IDj[i], IDd[i] ] - b_k[ IDk[i] ];
		p = inv_logit(v);
		y[i] ~ bernoulli(p);
	}
}
//# generated quantities{
//#     vector[N] log_lik;
//#     real v;
//#     real p;
//#     
//#     // likelihood
//#     for( i in 1:N ) {
//#       v = theta_sub[ IDj[i], IDd[i] ] - b_k[ IDk[i] ];
//#       p = inv_logit(v);
//#       log_lik[i] = bernoulli_lpmf( y[i] | p);
//#     }
//# }
\end{lstlisting}



\noindent \textbf{SOLV CP}
%
\begin{lstlisting}
data{
	// evaluation
	int N;
	int J;
	int K;
	int L;
	int D;
	int IDj[N];
	int IDk[N];
	int IDl[N];
	int IDd[N];
	int GE[N];
	real AG[N];
	int ED[N];
	int XP[N];
	int y[N];
	
	// individuals
	int IDind[J];
	int G[J];
	real A[J];
	int E[J];
	int X[J];
	
	// items
	int IDitem[K];
	int IDtext[K];
	int IDdim[K];
}
parameters{
	// items
	real m_b[L];
	real<lower=0> s_b[L];
	real b_k[K];
	
	// betas
	real a;
	real b_G[2];
	real b_A;
	real b_E[3];
	real b_X[4];
	
	// abilities
	vector[J] theta;              // reading comprehension
	real<lower=0> loads[D];       // loadings
	corr_matrix[D] Rho_theta_sub; // sub-dimensions: lit, inf, ref
	vector[D] theta_sub[J]; 
}
model{
	// declare
	vector[J] m_theta;
	vector[D] m_mult[J];
	real v;
	real p;
	
	// items
	m_b ~ normal(0, 1);
	s_b ~ exponential(2);
	for(k in 1:K){            // priors
		b_k[k] ~ normal( m_b[ IDtext[k] ], s_b[ IDtext[k] ]); 
	}
	
	// SOLV
	a ~ normal(0, 0.5);
	b_G ~ normal(0, 0.5);
	b_A ~ normal(0, 0.5);
	b_E ~ normal(0, 1);
	b_X ~ normal(0, 0.5);
	for(j in 1:J){
		m_theta[j] = a + b_G[ G[j] ] + 
			b_A * ( A[j] - min(A) ) +
			b_E[ E[j] ] + 
			b_X[ X[j] ];
	}
	theta ~ normal(m_theta, 1); 
	// setting the scale at this level
	
	// FOLV
	loads ~ lognormal(0, 0.5);
	for(j in 1:J){
		m_mult[j,] = [ loads[1]*theta[j], 
			loads[2]*theta[j], 
			loads[3]*theta[j] ]';
	}
	Rho_theta_sub ~ lkj_corr(2);
	theta_sub ~ multi_normal( m_mult, Rho_theta_sub ); 
	// also setting scale here
	
	// model
	for( i in 1:N ) {
		v = theta_sub[ IDj[i], IDd[i] ] - b_k[ IDk[i] ];
		p = inv_logit(v);
		y[i] ~ bernoulli(p);
	}
}
//# generated quantities{
//#     vector[N] log_lik;
//#     real v;
//#     real p;
//#     
//#     // likelihood
//#     for( i in 1:N ) {
//#       v = theta_sub[ IDj[i], IDd[i] ] - b_k[ IDk[i] ];
//#       p = inv_logit(v);
//#       log_lik[i] = bernoulli_lpmf( y[i] | p);
//#     }
//# }
\end{lstlisting}


\noindent \textbf{SOLV NCP}
%
\begin{lstlisting}
data{
	// evaluation
	int N;
	int J;
	int K;
	int L;
	int D;
	int IDj[N];
	int IDk[N];
	int IDl[N];
	int IDd[N];
	int GE[N];
	real AG[N];
	int ED[N];
	int XP[N];
	int y[N];
	
	// individuals
	int IDind[J];
	int G[J];
	real A[J];
	int E[J];
	int X[J];
	
	// items
	int IDitem[K];
	int IDtext[K];
	int IDdim[K];
}
parameters{
	// items
	real m_b[L];
	real<lower=0> s_b[L];
	real zb_k[K];
	
	// betas
	real a;
	real b_G[2];
	real b_A;
	real b_E[3];
	real b_X[4];
	
	// abilities
	vector[J] ztheta;             // reading comprehension
	real<lower=0> loads[D];       // loadings
	cholesky_factor_corr[D] L_Rho_theta_sub; // sub-dimensions
	matrix[D, J] ztheta_sub; 
}
transformed parameters{
	real b_k[K];
	vector[J] m_theta;
	vector[J] theta;             // reading comprehension
	matrix[D, D] Rho_theta_sub;
	matrix[J, D] m_mult;
	matrix[J, D] theta_sub;
	
	// items
	for(k in 1:K){
		b_k[k] = m_b[ IDtext[k] ] + s_b[ IDtext[k] ] * zb_k[k]; 
	}
	
	// SOLV
	for(j in 1:J){
		m_theta[j] = a + b_G[ G[j] ] + 
		b_A * ( A[j] - min(A) ) +
		b_E[ E[j] ] + 
		b_X[ X[j] ];
	}
	theta = m_theta + 1 * ztheta;
	// setting the scale at this level
	
	// FOLV
	Rho_theta_sub = multiply_lower_tri_self_transpose(L_Rho_theta_sub);
	for(j in 1:J){
		m_mult[j,] = to_row_vector( [ loads[1]*theta[j], 
		loads[2]*theta[j], 
		loads[3]*theta[j] ] );
	}
	theta_sub = m_mult + (L_Rho_theta_sub * ztheta_sub)';
	// also setting scale here
}
model{
	// declare
	real v;
	real p;
	
	// items
	m_b ~ normal(0, 1);
	s_b ~ exponential(2);
	zb_k ~ normal(0, 1);
	
	// abilities
	a ~ normal(0, 0.5);
	b_G ~ normal(0, 0.5);
	b_A ~ normal(0, 0.5);
	b_E ~ normal(0, 1);
	b_X ~ normal(0, 0.5);
	ztheta ~ normal(0,1);
	L_Rho_theta_sub ~ lkj_corr_cholesky(2);
	loads ~ lognormal(0, 0.5);
	to_vector(ztheta_sub) ~ normal(0,1);
	
	// model
	for( i in 1:N ) {
		v = theta_sub[ IDj[i], IDd[i] ] - b_k[ IDk[i] ];
		p = inv_logit(v);
		y[i] ~ bernoulli(p);
	}
}
//# generated quantities{
//#     vector[N] log_lik;
//#     real v;
//#     real p;
//#     
//#     // likelihood
//#     for( i in 1:N ) {
//#       v = theta_sub[ IDj[i], IDd[i] ] - b_k[ IDk[i] ];
//#       p = inv_logit(v);
//#       log_lik[i] = bernoulli_lpmf( y[i] | p);
//#     }
//# }
\end{lstlisting}


\subsubsection{Prior predictive investigation}

\noindent \textbf{Prior predictive}
%
\begin{lstlisting}
model_path = file.path(getwd(), 'models_prior')
model_list = list.files( model_path )
model_list = model_list[ str_detect(model_list, '.stan') ]

run_prior( models = model_list,
model_path = model_path,
model_out = file.path(getwd(), 'chains_prior'),
data_path = file.path(getwd(), 'data') )
\end{lstlisting}


\noindent \textbf{Prior predictive function}
%
\begin{lstlisting}
# function:
#     run_prior
# description:  
#     To run each prior simulation for each model
# arguments:
#     models = list of model to run located in model_path
#     model_path = path where all models are located
#     model_out = path where chains have to be saved
#     data_path = path where data per condition is located

run_prior = function(models, model_path, model_out, data_path){
	
# data list
data_list = list.files( data_path )
data_list = data_list[str_detect(data_list, 'ListFormat')]
	
for(j in 1:length(models) ){
		
	# compile model
	set_cmdstan_path('~/cmdstan')
	mod = cmdstan_model( file.path(model_path, models[j] ) )
	
	# generate base name
	base_nam = str_replace( data_list[1], 'ListFormat_', 
	str_replace( models[j], '.stan', '_') )
	base_nam = str_replace( base_nam, '.RData', '' )
		
	# load data
	load( file.path(data_path, data_list[1] ) )
		
	# run model
	mod$sample(data = data_post, 
		output_dir = model_out, 
		output_basename = base_nam,
		chains=1, parallel_chains=1, adapt_delta=0.99)
	}
}
\end{lstlisting}


\subsubsection{Posterior predictive}

\noindent \textbf{Posterior predictive}
%
\begin{lstlisting}
model_path = file.path(getwd(), 'models_post')
model_list = list.files( model_path )
model_list = model_list[ str_detect(model_list, '.stan') ]
model_list = model_list[c(1:2,5:6,3:4)]

run_post( models = model_list,
model_path = model_path,
model_out = file.path(getwd(), 'chains_post'),
data_path = file.path(getwd(), 'data') )
\end{lstlisting}


\noindent \textbf{Posterior predictive function}
%
\begin{lstlisting}
# function:
#     run_post
# description:  
#     To run each model for each data in all conditions
# arguments:
#     models = list of model to run located in model_path
#     model_path = path where all models are located
#     model_out = path where chains have to be saved
#     data_path = path where data per condition is located

run_post = function(models, model_path, model_out, data_path){
	
# data list
data_list = list.files( data_path )
data_list = data_list[str_detect(data_list, 'ListFormat')]
	
for(j in 1:length(models) ){
		
	# compile model
	set_cmdstan_path('~/cmdstan')
	mod = cmdstan_model( file.path(model_path, models[j] ) )
		
	for(i in 1:length(data_list) ){
			
		# generate base name
		base_nam = str_replace( data_list[i], 'ListFormat_', 
		str_replace(models[j], '.stan', '_') )
		base_nam = str_replace( base_nam, '.RData', '' )
			
		# load data
		load( file.path(data_path, data_list[i] ) )
			
		# run model
		t0 = proc.time()
		mod$sample(data = data_post, 
			output_dir = model_out, 
			output_basename = base_nam,
			chains=3, parallel_chains=3, adapt_delta=0.99)
		t1 = proc.time()
			
		# saving time
		start = str_locate(base_nam, 'J')[2]
		m = str_sub( base_nam, start=1, end=(start-2) )
		J = str_sub( base_nam, start=start+1, end=(start+3) )
			
		start = str_locate(base_nam, 'l')[2] 
		l = str_sub( base_nam, start=start+1, end=(start+3) )
		
		start = str_locate(base_nam, 'Ndata')[2]
		S = str_sub( base_nam, start=start+1, end=(start+2) )
		
		elapsed = (t1-t0)
			
		if(j==1 & i==1){
			time_elap = data.frame(
				Model=m, 
				J=J, 
				load=l, 
				data=S, 
				time=unlist(elapsed['elapsed']) )
		} else{
			time_elap = rbind(time_elap, 
				c(m, J, l, S, 
					unlist(elapsed['elapsed']) ) )
		}
			
		# save time elapsed (saved at each iteration)
		write.csv( time_elap, row.names=F, 
		file=file.path(model_out, 'time_elapsed.csv' ) )
			
	}
}

}
\end{lstlisting}