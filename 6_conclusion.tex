\chapter{Conclusions and discussion} \label{cap:conclusions}

As stated in the introductory section, the current research described and implemented the Bayesian GLLAMM model for dichotomous outcomes \cite{Rabe_et_al_2004a, Rabe_et_al_2004b, Skrondal_et_al_2004a, Rabe_et_al_2012}, in the context of an educational data. The reason for the model's proposal revolved around the fact that educational data often presents multiple types of dependencies, that left unchecked, can cause IRT models to violate their assumptions of local independence. The latter is particularly important, as violation of these assumption prevent IRT models to reach appropriate inferences from the parameter estimates \cite{Yen_1984, Chen_et_al_1997, Jiao_et_al_2012}.

Moreover, in the context of the previously defined model, the current research also provided an assessment of the benefits resulting from changing the posterior sampling geometries. Multiple evidence pointed out the performance improvement on the MCMC methods from using non-centered parameterizations \cite{Gelfand_et_al_1995, Gelfand_et_al_1996, Papaspiliopoulos_et_al_2003, Papaspiliopoulos_et_al_2007, Betancourt_et_al_2013}. However, most of the evidence have been developed under Gaussian hierarchical models. So, it seemed sensible to provide a similar assessment for nonlinear latent stochastic models, like our implementation \cite{Papaspiliopoulos_et_al_2007}.

Finally, the research applied the newfound knowledge to a large standardized teacher assessments from Peru. The purpose of the latter was to evaluate the change of parametrization on a real data setting, determine the evidence in favor of our models of interest, produce psychometric analysis, and finally assess specific research hypothesis. \\

\noindent Therefore the main conclusion derived from our work were the followings:
%
\begin{enumerate}
	%
	\item In the context of the implemented model, the non-centered parametrization largely improved the performance of the MCMC chains, towards achieving ergodicity. This was true across models, simulated sample sizes, simulated replicas, and even under the real application, albeit with some caveats. 

	The most important caveat, under the simulation and application setting, was that no matter the parametrization, no large difference in performance was observed in either the sub-dimensions' correlation or loading parameters. On this matter, however, no evidence supported the idea the parameters suffered from a further lack of identification.
	%
	\item Our proposed model was able to recover most of the simulated parameters with good precision.
	
	Similar to the previous result, the model still had issues estimating the sub-dimensions correlations and loadings. This result is important, as under the Confirmatory Factor Analysis theory (CFA), a SOLV model is only justified, if the lower-level correlations are high enough (usually above $0.8$). Moreover, according to the same theory, once the SOLV model is fitted, assuming the model is correct, it is expected the correlation of the lower-level latent variables to be largely reduced, something that was not observed in our simulation studies, not even in accordance to our simulation parameters.
	%
	\item The proposed models managed to produce a rather well depiction of the true simulated ICC and IIF curves. 
	
	The previous implied the models allow us to correctly recover the item's psychometric characteristics, a trait of high relevance for the development of evaluation instruments.
	%
	\item In terms of retrodiction accuracy, the models managed to capture the traits of the data, while avoiding its exact replication. These result were consistent across models, simulated sample sizes and replicas, and even under the final educational application. 
	
	Consequently, it is safe to say the models produced similar encodings of the data, leaving the decision of choosing one model over the other, on a more theoretical ground.
	%
	\item The non-centered parametrization was slightly faster than the centered counterpart, although the magnitudes of the differences in running time were not large. 
	
	This result is still important, as the non-centered parametrization was more complex, and required the sampling of more parameters than the centered counterpart. This mean that improving the performance of the MCMC, through a more complex model as the NCP, did not come with a cost on running time.
	%
	\item On the application side, the model provided an extra benefit, that is, we were able to asses the psychometric properties of texts, rather than just items.
	%
	\item Finally, in relation to our hypothesis of interest, the model was able to produce sound statistical results, supported not only by the statistical application, but also from a DAG guiding our interpretation and causal assumptions.
	%
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Future developments}

From the simulation studies and application, we noticed the benefits of the non-centered parametrization did not extend to the estimation of correlation parameters or loadings. In this regard, it would be of interest to investigate why we observe this pattern, considering that the hypothesis of lack of identification have been discarded.

On the other hand, in similar sections, we have been surprised that although the chains produced by the centered parametrization, did not show signs of achieving ergodicity, its recovery capacity was at par of its non-centered counterpart. The considered hypothesis were related to the use of the HMC algorithm with a higher rejection criteria (\texttt{adapt\_delta}$=0.99$) and weakly regularizing priors. However, it would be interesting to test the validity of these hypothesis, using the full factorial design outlined in chapter $4$, and even adding a new factor to the mix, that is, a comparison between HMC and Gibbs sampling.

Related to the previous two statements, evidence on similar chapters have revealed that, some parameters within a model were sampled with better performance with the CP, in contrast to the NCP, while in the remaining parameters happened the opposite. This resonates with the statement of \citet{Papaspiliopoulos_et_al_2007}, that the advantages of the NCP strategy largely depends on the specifics of the model and data, and that the CP and NCP are complements of each other, rather than replacements. Therefore, under this scenario, it seems sensible to investigate the benefits of Variational Inference methods (VI) to estimate the posterior distribution of the parameters. As the reader can recall, VI seek to produce a sample mechanism located in a continuous between a CP and NCP.

On the other hand, in the hypothesis test section of the application chapter, we realized that a simple random sample was not the appropriate sample design to evaluate all our contrasts of interest. This was specially true in variables with levels that had low representativeness, like disability. Furthermore, we realized that the full benefit of a DAG, comes from using it not only to structure our statistical model, but also to design the collection of data. In this sense, it would be enlightening to see if the hypothesis results are replicated with a more appropriate sample design. 

Furthermore, in our application section, we assumed the relationship between the reading comprehension abilities and age was linear. However, nothing prevents that non-linearities are present in this relationship. In a similar spirit, it would be interesting to apply the GLLAMM model, in a setting were covariates are not only at the structural level, but also at the level of the responses, e.g. assessment in online environments.

Finally, remains as an interesting path for future research, the application of the GLLAMM to a multi-group setting, where an analysis of invariance is required.

